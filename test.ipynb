{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1462, 0.6839, 0.2889, 0.2769],\n",
       "         [0.7806, 0.4126, 0.0584, 0.0899],\n",
       "         [0.1423, 0.6661, 0.2017, 0.1897],\n",
       "         [0.4954, 0.1343, 0.0377, 0.0950],\n",
       "         [0.4384, 0.4964, 0.0300, 0.0708]]),\n",
       " ['cells(0.58)', 'cells(0.32)', 'cells(0.30)', 'cells(0.23)', 'cells(0.22)'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def extract_confidences(pred_phrases):\n",
    "    return torch.tensor([float(p.split('(')[-1].rstrip(')')) for p in pred_phrases])\n",
    "\n",
    "def xywh_to_xyxy(boxes):\n",
    "    # Convert [cx, cy, w, h] to [x1, y1, x2, y2]\n",
    "    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    x1 = cx - w / 2\n",
    "    y1 = cy - h / 2\n",
    "    x2 = cx + w / 2\n",
    "    y2 = cy + h / 2\n",
    "    return torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "def compute_iou(box1, boxes2):\n",
    "    # Compute IoU between one box and multiple boxes\n",
    "    x1 = torch.max(box1[0], boxes2[:, 0])\n",
    "    y1 = torch.max(box1[1], boxes2[:, 1])\n",
    "    x2 = torch.min(box1[2], boxes2[:, 2])\n",
    "    y2 = torch.min(box1[3], boxes2[:, 3])\n",
    "    \n",
    "    inter_area = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    \n",
    "    union_area = area1 + area2 - inter_area\n",
    "    return inter_area / union_area\n",
    "\n",
    "def filter_by_iou_and_confidence(boxes_filt, pred_phrases, iou_threshold=0.5, large_box_area_thresh=0.9):\n",
    "    if boxes_filt.size(0) == 0:\n",
    "        return boxes_filt, pred_phrases\n",
    "\n",
    "    confidences = extract_confidences(pred_phrases)\n",
    "    boxes_xyxy = xywh_to_xyxy(boxes_filt)\n",
    "    areas = boxes_filt[:, 2] * boxes_filt[:, 3]\n",
    "\n",
    "    # Optional: filter out extremely large boxes\n",
    "    not_large = (boxes_filt[:, 2] < large_box_area_thresh) & (boxes_filt[:, 3] < large_box_area_thresh)\n",
    "    boxes_filt = boxes_filt[not_large]\n",
    "    boxes_xyxy = boxes_xyxy[not_large]\n",
    "    confidences = confidences[not_large]\n",
    "    pred_phrases = [pred_phrases[i] for i in torch.where(not_large)[0]]\n",
    "\n",
    "    keep = torch.ones(len(boxes_filt), dtype=torch.bool)\n",
    "\n",
    "    for i in range(len(boxes_filt)):\n",
    "        if not keep[i]:\n",
    "            continue\n",
    "        iou = compute_iou(boxes_xyxy[i], boxes_xyxy)\n",
    "        overlapping_idxs = torch.where((iou > iou_threshold) & (iou < 1.0))[0]\n",
    "\n",
    "        for j in overlapping_idxs:\n",
    "            if confidences[i] >= confidences[j]:\n",
    "                keep[j] = False\n",
    "            else:\n",
    "                keep[i] = False\n",
    "\n",
    "    boxes_final = boxes_filt[keep]\n",
    "    phrases_final = [pred_phrases[i] for i in torch.where(keep)[0]]\n",
    "\n",
    "    return boxes_final, phrases_final\n",
    "\n",
    "boxes = torch.tensor([\n",
    "    [0.1462, 0.6839, 0.2889, 0.2769],\n",
    "    [0.7806, 0.4126, 0.0584, 0.0899],\n",
    "    [0.1423, 0.6661, 0.2017, 0.1897],\n",
    "    [0.5000, 0.4993, 0.9953, 0.9976],\n",
    "    [0.4954, 0.1343, 0.0377, 0.0950],\n",
    "    [0.4384, 0.4964, 0.0300, 0.0708],\n",
    "])\n",
    "phrases = ['cells(0.58)', 'cells(0.32)', 'cells(0.30)', 'cells(0.32)', 'cells(0.23)', 'cells(0.22)']\n",
    "\n",
    "filtered_boxes, filtered_phrases = filter_by_iou_and_confidence(boxes, phrases)\n",
    "filtered_boxes, filtered_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered boxes:\n",
      " tensor([[0.1462, 0.6839, 0.2889, 0.2769],\n",
      "        [0.7806, 0.4126, 0.0584, 0.0899],\n",
      "        [0.5000, 0.4993, 0.9953, 0.9976],\n",
      "        [0.4954, 0.1343, 0.0377, 0.0950],\n",
      "        [0.4384, 0.4964, 0.0300, 0.0708]])\n",
      "Filtered phrases:\n",
      " ['cells(0.58)', 'cells(0.32)', 'cells(0.32)', 'cells(0.23)', 'cells(0.22)']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "boxes = torch.tensor([\n",
    "    [0.1462, 0.6839, 0.2889, 0.2769],\n",
    "    [0.7806, 0.4126, 0.0584, 0.0899],\n",
    "    [0.1423, 0.6661, 0.2017, 0.1897],\n",
    "    [0.5000, 0.4993, 0.9953, 0.9976],\n",
    "    [0.4954, 0.1343, 0.0377, 0.0950],\n",
    "    [0.4384, 0.4964, 0.0300, 0.0708],\n",
    "])\n",
    "phrases = ['cells(0.58)', 'cells(0.32)', 'cells(0.30)', 'cells(0.32)', 'cells(0.23)', 'cells(0.22)']\n",
    "\n",
    "def extract_confidences(pred_phrases):\n",
    "    return torch.tensor([float(p.split('(')[-1].rstrip(')')) for p in pred_phrases])\n",
    "\n",
    "def xywh_to_xyxy(boxes):\n",
    "    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    x1 = cx - w / 2\n",
    "    y1 = cy - h / 2\n",
    "    x2 = cx + w / 2\n",
    "    y2 = cy + h / 2\n",
    "    return torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - inter_area\n",
    "    return inter_area / union if union > 0 else 0\n",
    "\n",
    "def nms_by_confidence(boxes, phrases, iou_threshold=0.3):\n",
    "    confidences = extract_confidences(phrases)\n",
    "    boxes_xyxy = xywh_to_xyxy(boxes)\n",
    "    indices = sorted(range(len(confidences)), key=lambda i: confidences[i], reverse=True)\n",
    "\n",
    "    keep = []\n",
    "    removed = set()\n",
    "\n",
    "    for i in indices:\n",
    "        if i in removed:\n",
    "            continue\n",
    "        keep.append(i)\n",
    "        for j in indices:\n",
    "            if j != i and j not in removed:\n",
    "                iou = compute_iou(boxes_xyxy[i], boxes_xyxy[j])\n",
    "                if iou > iou_threshold:\n",
    "                    removed.add(j)\n",
    "\n",
    "    filtered_boxes = boxes[keep]\n",
    "    filtered_phrases = [phrases[i] for i in keep]\n",
    "    return filtered_boxes, filtered_phrases\n",
    "\n",
    "# Apply filtering\n",
    "filtered_boxes, filtered_phrases = nms_by_confidence(boxes, phrases)\n",
    "\n",
    "print(\"Filtered boxes:\\n\", filtered_boxes)\n",
    "print(\"Filtered phrases:\\n\", filtered_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014210224151611328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading model.safetensors",
       "rate": null,
       "total": 440449768,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5922f34578364d36afa13de9f01254bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"../04-06-segment-anything/weights/groundingdino_swint_ogc.pth\")\n",
    "IMAGE_PATH = \".asset/cat_dog.jpeg\"\n",
    "TEXT_PROMPT = \"chair . person . dog .\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from glob import glob\n",
    "from time import time\n",
    "import pytesseract\n",
    "import random  \n",
    "import re\n",
    "from PIL import ImageOps\n",
    "\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.vl_utils import create_positive_map_from_span\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "\n",
    "class GroundingDINOApp:\n",
    "    def __init__(self, config_path, checkpoint_path, device, cpu_only=False):\n",
    "        self.cpu_only = cpu_only        \n",
    "        self.device   = device\n",
    "        self.model = self.load_model(config_path, checkpoint_path)\n",
    "\n",
    "    def load_model(self, config_path, checkpoint_path):\n",
    "        args = SLConfig.fromfile(config_path)\n",
    "        args.device = self.device\n",
    "        model = build_model(args)\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "        model.eval()\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def preprocess_image(self, image_pil):\n",
    "        transform = T.Compose([\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        image, _ = transform(image_pil, None)\n",
    "        return image_pil, image.to(self.device)\n",
    "\n",
    "    def plot_boxes(self, image_pil, boxes, labels):\n",
    "        W, H = image_pil.size\n",
    "        draw = ImageDraw.Draw(image_pil)\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "        for box, label in zip(boxes, labels):\n",
    "            box = box * torch.tensor([W, H, W, H])\n",
    "            box[:2] -= box[2:] / 2\n",
    "            box[2:] += box[:2]\n",
    "            x0, y0, x1, y1 = box.int().tolist()\n",
    "            color = tuple(np.random.randint(0, 255, size=3).tolist())\n",
    "            draw.rectangle([x0, y0, x1, y1], outline=color, width=4)\n",
    "            draw.text((x0, y0), label, fill=\"white\", font=font)\n",
    "        return image_pil\n",
    "\n",
    "    def get_grounding_output(self, image_tensor, caption, box_thresh, text_thresh):\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        caption = caption.strip().lower()\n",
    "        if not caption.endswith(\".\"):\n",
    "            caption += \".\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image_tensor[None], captions=[caption])\n",
    "\n",
    "        logits = outputs[\"pred_logits\"].sigmoid()[0]\n",
    "        boxes = outputs[\"pred_boxes\"][0]\n",
    "\n",
    "        filt_mask = logits.max(dim=1)[0] > box_thresh\n",
    "        logits_filt = logits[filt_mask]\n",
    "        boxes_filt = boxes[filt_mask]\n",
    "\n",
    "        tokenized = self.model.tokenizer(caption)\n",
    "        pred_phrases = [\n",
    "            get_phrases_from_posmap(logit > text_thresh, tokenized, self.model.tokenizer) +\n",
    "            f\" ({logit.max().item():.2f})\"\n",
    "            for logit in logits_filt\n",
    "        ]\n",
    "        return boxes_filt, pred_phrases\n",
    "\n",
    "    def crop_and_ocr(self, original_image, box):\n",
    "        H, W, _ = original_image.shape\n",
    "        cx, cy, bw, bh = box.tolist()\n",
    "\n",
    "        x1 = int((cx - bw / 2) * W)\n",
    "        y1 = int((cy - bh / 2) * H)\n",
    "        x2 = int((cx + bw / 2) * W)\n",
    "        y2 = int((cy + bh / 2) * H)\n",
    "\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(W, x2), min(H, y2)\n",
    "\n",
    "        cropped = original_image[y1:y2, x1:x2]\n",
    "        text = pytesseract.image_to_string(cropped, config='--psm 7')  \n",
    "        \n",
    "        return cropped, text.strip()\n",
    "    \n",
    "# Language selection\n",
    "st.set_page_config(page_title=\"Grounding DINO Streamlit Demo\", layout=\"centered\")\n",
    "lang = st.sidebar.selectbox(\"üåê Select Language / Ïñ∏Ïñ¥ ÏÑ†ÌÉù\", [\"Korean\", \"English\"])\n",
    "\n",
    "# Language-specific text\n",
    "if lang == \"Korean\":\n",
    "    st.title(\"üîç Grounding DINO Îç∞Î™®\")\n",
    "    st.write(\"Ïù¥ÎØ∏ÏßÄÎ•º ÏóÖÎ°úÎìúÌïòÍ≥† ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏Ïóê Îî∞Îùº Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌï¥Î≥¥ÏÑ∏Ïöî.\")\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.header(\"ÏÑ§Ï†ï\")\n",
    "        config_path = st.text_input(\"ÏÑ§Ï†ï ÌååÏùº Í≤ΩÎ°ú\", \"groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "        checkpoint_path = st.text_input(\"Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùº Í≤ΩÎ°ú\", \"/home/bekhzod/Desktop/localization_models_performance/weights/groundingdino_swint_ogc.pth\")\n",
    "        cpu_only = st.checkbox(\"CPUÎßå ÏÇ¨Ïö©\", value=False)\n",
    "        box_thresh = st.slider(\"Î∞ïÏä§ ÏûÑÍ≥ÑÍ∞í\", 0.0, 1.0, 0.3, 0.05)\n",
    "        text_thresh = st.slider(\"ÌÖçÏä§Ìä∏ ÏûÑÍ≥ÑÍ∞í\", 0.0, 1.0, 0.3, 0.05)\n",
    "\n",
    "    st.title(\"üß† Ï∞®Îüâ Î≤àÌò∏Ìåê Ïù∏Ïãù Ïï±\")\n",
    "    text_prompt = st.text_input(\"ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏\", \"Î≤àÌò∏Ìåê\")\n",
    "    if text_prompt == \"Î≤àÌò∏Ìåê\": text_prompt = \"license plate\"\n",
    "\n",
    "    uploaded_image = st.file_uploader(\"ÎòêÎäî Ïù¥ÎØ∏ÏßÄ ÏóÖÎ°úÎìú\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
    "    image_dir = st.text_input(\"Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî Í≤ΩÎ°ú (ÏÑ†ÌÉù ÏÇ¨Ìï≠)\", \"/home/bekhzod/Desktop/localization_models_performance/lp_images/\")\n",
    "\n",
    "else:  # English interface\n",
    "    st.title(\"üîç Grounding DINO Demo\")\n",
    "    st.write(\"Upload an image and detect objects based on your text prompt.\")\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.header(\"Settings\")\n",
    "        config_path = st.text_input(\"Configuration File Path\", \"groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "        checkpoint_path = st.text_input(\"Checkpoint File Path\", \"weights/groundingdino_swint_ogc.pth\")\n",
    "        cpu_only = st.checkbox(\"Use CPU only\", value=False)\n",
    "        box_thresh = st.slider(\"Box Threshold\", 0.0, 1.0, 0.3, 0.05)\n",
    "        text_thresh = st.slider(\"Text Threshold\", 0.0, 1.0, 0.3, 0.05)\n",
    "\n",
    "    st.title(\"üß† Vehicle License Plate Recognition App\")\n",
    "    text_prompt = st.text_input(\"Text Prompt\", \"license plate\")\n",
    "\n",
    "    uploaded_image = st.file_uploader(\"Or upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
    "    image_dir = st.text_input(\"Image Folder Path (Optional)\", \"/home/bekhzod/Desktop/localization_models_performance/lp_images/\")\n",
    "\n",
    "# Initialize model\n",
    "device = \"cpu\" if cpu_only else \"cuda\"\n",
    "\n",
    "if os.path.exists(config_path) and os.path.exists(checkpoint_path):\n",
    "    g_dino = GroundingDINOApp(config_path = config_path, checkpoint_path = checkpoint_path, cpu_only = cpu_only, device = device)\n",
    "else:\n",
    "    st.error(\"Please provide valid configuration and checkpoint file paths.\" if lang == \"English\" else \"Ïú†Ìö®Ìïú ÏÑ§Ï†ï ÌååÏùº Î∞è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°úÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.\")\n",
    "    st.stop()\n",
    "\n",
    "# Image preview and selection\n",
    "detection_triggered = False\n",
    "detection_image = None\n",
    "original_cv2 = None\n",
    "result_image = None\n",
    "cropped_img = None\n",
    "ocr_text = \"\"\n",
    "\n",
    "if os.path.isdir(image_dir):\n",
    "    image_paths = glob(os.path.join(image_dir, \"*.[jp][pn]g\"))\n",
    "    random.shuffle(image_paths)\n",
    "    selected_images = image_paths[:10]\n",
    "\n",
    "    st.markdown(\"### üñºÔ∏è Random Image Preview\" if lang == \"English\" else \"### üñºÔ∏è ÎûúÎç§ Ïù¥ÎØ∏ÏßÄ ÎØ∏Î¶¨Î≥¥Í∏∞\")\n",
    "    rows = [selected_images[i:i+5] for i in range(0, len(selected_images), 5)]\n",
    "    for row in rows:\n",
    "        cols = st.columns(5)\n",
    "        for col, img_path in zip(cols, row):\n",
    "            with col:\n",
    "                pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "                pil_img = ImageOps.fit(pil_img, (200, 200))\n",
    "                st.image(pil_img, caption=os.path.basename(img_path), use_container_width=False)\n",
    "                if st.button(\"Detect from Image\" if lang == \"English\" else \"ÏúÑ Ïù¥ÎØ∏ÏßÄ ÌÉêÏßÄÌïòÍ∏∞\", key=img_path):\n",
    "                    detection_triggered = True\n",
    "                    detection_image = Image.open(img_path).convert(\"RGB\")\n",
    "                    original_cv2 = np.array(detection_image)\n",
    "elif image_dir.strip():\n",
    "    st.warning(\"Invalid image folder path or folder is empty.\" if lang == \"English\" else \"ÏûÖÎ†•Ìïú Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî Í≤ΩÎ°úÍ∞Ä ÏûòÎ™ªÎêòÏóàÍ±∞ÎÇò Ìè¥ÎçîÍ∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§.\")\n",
    "\n",
    "if uploaded_image and not detection_triggered:\n",
    "    detection_image = Image.open(uploaded_image).convert(\"RGB\")\n",
    "    original_cv2 = np.array(detection_image)\n",
    "    detection_triggered = True\n",
    "\n",
    "if detection_triggered and detection_image is not None:\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### üîç Detection Results\" if lang == \"English\" else \"### üîç ÌÉêÏßÄ Í≤∞Í≥º\")\n",
    "\n",
    "    st.image(detection_image, caption=\"Original Image\" if lang == \"English\" else \"ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ\", use_container_width=True)\n",
    "\n",
    "    with st.spinner(\"Detecting...\" if lang == \"English\" else \"ÌÉêÏßÄ Ï§ë...\"):\n",
    "        _, image_tensor = g_dino.preprocess_image(detection_image)\n",
    "        boxes, phrases = g_dino.get_grounding_output(image_tensor, text_prompt, box_thresh, text_thresh)\n",
    "        boxes = boxes.to(\"cpu\")\n",
    "\n",
    "        result_image = g_dino.plot_boxes(detection_image.copy(), boxes, phrases)\n",
    "        st.image(result_image, caption=\"Detected Results\" if lang == \"English\" else \"ÌÉêÏßÄÎêú Í≤∞Í≥º\", use_container_width=True)\n",
    "\n",
    "        if len(boxes) > 0:\n",
    "            cropped_img, ocr_text = g_dino.crop_and_ocr(original_cv2, boxes[0])\n",
    "            st.subheader(\"üìù OCR Result\" if lang == \"English\" else \"üìù OCR Í≤∞Í≥º\")\n",
    "            st.image(cropped_img, caption=\"Cropped Region\" if lang == \"English\" else \"ÏûòÎùºÎÇ∏ ÏòÅÏó≠\", use_container_width=True)\n",
    "\n",
    "            cleaned_text = re.sub(r'[^A-Za-z0-9\\- ]', '', ocr_text)\n",
    "            st.success(f\"OCR Result: {cleaned_text}\" if lang == \"English\" else f\"OCR Ïù∏Ïãù Í≤∞Í≥º: {cleaned_text}\")\n",
    "        else:\n",
    "            st.warning(\"No object detected.\" if lang == \"English\" else \"ÌÉêÏßÄÎêú Í∞ùÏ≤¥Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localization",
   "language": "python",
   "name": "localization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
