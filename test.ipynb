{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1462, 0.6839, 0.2889, 0.2769],\n",
       "         [0.7806, 0.4126, 0.0584, 0.0899],\n",
       "         [0.1423, 0.6661, 0.2017, 0.1897],\n",
       "         [0.4954, 0.1343, 0.0377, 0.0950],\n",
       "         [0.4384, 0.4964, 0.0300, 0.0708]]),\n",
       " ['cells(0.58)', 'cells(0.32)', 'cells(0.30)', 'cells(0.23)', 'cells(0.22)'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def extract_confidences(pred_phrases):\n",
    "    return torch.tensor([float(p.split('(')[-1].rstrip(')')) for p in pred_phrases])\n",
    "\n",
    "def xywh_to_xyxy(boxes):\n",
    "    # Convert [cx, cy, w, h] to [x1, y1, x2, y2]\n",
    "    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    x1 = cx - w / 2\n",
    "    y1 = cy - h / 2\n",
    "    x2 = cx + w / 2\n",
    "    y2 = cy + h / 2\n",
    "    return torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "def compute_iou(box1, boxes2):\n",
    "    # Compute IoU between one box and multiple boxes\n",
    "    x1 = torch.max(box1[0], boxes2[:, 0])\n",
    "    y1 = torch.max(box1[1], boxes2[:, 1])\n",
    "    x2 = torch.min(box1[2], boxes2[:, 2])\n",
    "    y2 = torch.min(box1[3], boxes2[:, 3])\n",
    "    \n",
    "    inter_area = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    \n",
    "    union_area = area1 + area2 - inter_area\n",
    "    return inter_area / union_area\n",
    "\n",
    "def filter_by_iou_and_confidence(boxes_filt, pred_phrases, iou_threshold=0.5, large_box_area_thresh=0.9):\n",
    "    if boxes_filt.size(0) == 0:\n",
    "        return boxes_filt, pred_phrases\n",
    "\n",
    "    confidences = extract_confidences(pred_phrases)\n",
    "    boxes_xyxy = xywh_to_xyxy(boxes_filt)\n",
    "    areas = boxes_filt[:, 2] * boxes_filt[:, 3]\n",
    "\n",
    "    # Optional: filter out extremely large boxes\n",
    "    not_large = (boxes_filt[:, 2] < large_box_area_thresh) & (boxes_filt[:, 3] < large_box_area_thresh)\n",
    "    boxes_filt = boxes_filt[not_large]\n",
    "    boxes_xyxy = boxes_xyxy[not_large]\n",
    "    confidences = confidences[not_large]\n",
    "    pred_phrases = [pred_phrases[i] for i in torch.where(not_large)[0]]\n",
    "\n",
    "    keep = torch.ones(len(boxes_filt), dtype=torch.bool)\n",
    "\n",
    "    for i in range(len(boxes_filt)):\n",
    "        if not keep[i]:\n",
    "            continue\n",
    "        iou = compute_iou(boxes_xyxy[i], boxes_xyxy)\n",
    "        overlapping_idxs = torch.where((iou > iou_threshold) & (iou < 1.0))[0]\n",
    "\n",
    "        for j in overlapping_idxs:\n",
    "            if confidences[i] >= confidences[j]:\n",
    "                keep[j] = False\n",
    "            else:\n",
    "                keep[i] = False\n",
    "\n",
    "    boxes_final = boxes_filt[keep]\n",
    "    phrases_final = [pred_phrases[i] for i in torch.where(keep)[0]]\n",
    "\n",
    "    return boxes_final, phrases_final\n",
    "\n",
    "boxes = torch.tensor([\n",
    "    [0.1462, 0.6839, 0.2889, 0.2769],\n",
    "    [0.7806, 0.4126, 0.0584, 0.0899],\n",
    "    [0.1423, 0.6661, 0.2017, 0.1897],\n",
    "    [0.5000, 0.4993, 0.9953, 0.9976],\n",
    "    [0.4954, 0.1343, 0.0377, 0.0950],\n",
    "    [0.4384, 0.4964, 0.0300, 0.0708],\n",
    "])\n",
    "phrases = ['cells(0.58)', 'cells(0.32)', 'cells(0.30)', 'cells(0.32)', 'cells(0.23)', 'cells(0.22)']\n",
    "\n",
    "filtered_boxes, filtered_phrases = filter_by_iou_and_confidence(boxes, phrases)\n",
    "filtered_boxes, filtered_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered boxes:\n",
      " tensor([[0.1462, 0.6839, 0.2889, 0.2769],\n",
      "        [0.7806, 0.4126, 0.0584, 0.0899],\n",
      "        [0.5000, 0.4993, 0.9953, 0.9976],\n",
      "        [0.4954, 0.1343, 0.0377, 0.0950],\n",
      "        [0.4384, 0.4964, 0.0300, 0.0708]])\n",
      "Filtered phrases:\n",
      " ['cells(0.58)', 'cells(0.32)', 'cells(0.32)', 'cells(0.23)', 'cells(0.22)']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "boxes = torch.tensor([\n",
    "    [0.1462, 0.6839, 0.2889, 0.2769],\n",
    "    [0.7806, 0.4126, 0.0584, 0.0899],\n",
    "    [0.1423, 0.6661, 0.2017, 0.1897],\n",
    "    [0.5000, 0.4993, 0.9953, 0.9976],\n",
    "    [0.4954, 0.1343, 0.0377, 0.0950],\n",
    "    [0.4384, 0.4964, 0.0300, 0.0708],\n",
    "])\n",
    "phrases = ['cells(0.58)', 'cells(0.32)', 'cells(0.30)', 'cells(0.32)', 'cells(0.23)', 'cells(0.22)']\n",
    "\n",
    "def extract_confidences(pred_phrases):\n",
    "    return torch.tensor([float(p.split('(')[-1].rstrip(')')) for p in pred_phrases])\n",
    "\n",
    "def xywh_to_xyxy(boxes):\n",
    "    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    x1 = cx - w / 2\n",
    "    y1 = cy - h / 2\n",
    "    x2 = cx + w / 2\n",
    "    y2 = cy + h / 2\n",
    "    return torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - inter_area\n",
    "    return inter_area / union if union > 0 else 0\n",
    "\n",
    "def nms_by_confidence(boxes, phrases, iou_threshold=0.3):\n",
    "    confidences = extract_confidences(phrases)\n",
    "    boxes_xyxy = xywh_to_xyxy(boxes)\n",
    "    indices = sorted(range(len(confidences)), key=lambda i: confidences[i], reverse=True)\n",
    "\n",
    "    keep = []\n",
    "    removed = set()\n",
    "\n",
    "    for i in indices:\n",
    "        if i in removed:\n",
    "            continue\n",
    "        keep.append(i)\n",
    "        for j in indices:\n",
    "            if j != i and j not in removed:\n",
    "                iou = compute_iou(boxes_xyxy[i], boxes_xyxy[j])\n",
    "                if iou > iou_threshold:\n",
    "                    removed.add(j)\n",
    "\n",
    "    filtered_boxes = boxes[keep]\n",
    "    filtered_phrases = [phrases[i] for i in keep]\n",
    "    return filtered_boxes, filtered_phrases\n",
    "\n",
    "# Apply filtering\n",
    "filtered_boxes, filtered_phrases = nms_by_confidence(boxes, phrases)\n",
    "\n",
    "print(\"Filtered boxes:\\n\", filtered_boxes)\n",
    "print(\"Filtered phrases:\\n\", filtered_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014210224151611328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading model.safetensors",
       "rate": null,
       "total": 440449768,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5922f34578364d36afa13de9f01254bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"../04-06-segment-anything/weights/groundingdino_swint_ogc.pth\")\n",
    "IMAGE_PATH = \".asset/cat_dog.jpeg\"\n",
    "TEXT_PROMPT = \"chair . person . dog .\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from glob import glob\n",
    "from time import time\n",
    "import pytesseract\n",
    "import random  \n",
    "import re\n",
    "from PIL import ImageOps\n",
    "\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.vl_utils import create_positive_map_from_span\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "\n",
    "class GroundingDINOApp:\n",
    "    def __init__(self, config_path, checkpoint_path, device, cpu_only=False):\n",
    "        self.cpu_only = cpu_only        \n",
    "        self.device   = device\n",
    "        self.model = self.load_model(config_path, checkpoint_path)\n",
    "\n",
    "    def load_model(self, config_path, checkpoint_path):\n",
    "        args = SLConfig.fromfile(config_path)\n",
    "        args.device = self.device\n",
    "        model = build_model(args)\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "        model.eval()\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def preprocess_image(self, image_pil):\n",
    "        transform = T.Compose([\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        image, _ = transform(image_pil, None)\n",
    "        return image_pil, image.to(self.device)\n",
    "\n",
    "    def plot_boxes(self, image_pil, boxes, labels):\n",
    "        W, H = image_pil.size\n",
    "        draw = ImageDraw.Draw(image_pil)\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "        for box, label in zip(boxes, labels):\n",
    "            box = box * torch.tensor([W, H, W, H])\n",
    "            box[:2] -= box[2:] / 2\n",
    "            box[2:] += box[:2]\n",
    "            x0, y0, x1, y1 = box.int().tolist()\n",
    "            color = tuple(np.random.randint(0, 255, size=3).tolist())\n",
    "            draw.rectangle([x0, y0, x1, y1], outline=color, width=4)\n",
    "            draw.text((x0, y0), label, fill=\"white\", font=font)\n",
    "        return image_pil\n",
    "\n",
    "    def get_grounding_output(self, image_tensor, caption, box_thresh, text_thresh):\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        caption = caption.strip().lower()\n",
    "        if not caption.endswith(\".\"):\n",
    "            caption += \".\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image_tensor[None], captions=[caption])\n",
    "\n",
    "        logits = outputs[\"pred_logits\"].sigmoid()[0]\n",
    "        boxes = outputs[\"pred_boxes\"][0]\n",
    "\n",
    "        filt_mask = logits.max(dim=1)[0] > box_thresh\n",
    "        logits_filt = logits[filt_mask]\n",
    "        boxes_filt = boxes[filt_mask]\n",
    "\n",
    "        tokenized = self.model.tokenizer(caption)\n",
    "        pred_phrases = [\n",
    "            get_phrases_from_posmap(logit > text_thresh, tokenized, self.model.tokenizer) +\n",
    "            f\" ({logit.max().item():.2f})\"\n",
    "            for logit in logits_filt\n",
    "        ]\n",
    "        return boxes_filt, pred_phrases\n",
    "\n",
    "    def crop_and_ocr(self, original_image, box):\n",
    "        H, W, _ = original_image.shape\n",
    "        cx, cy, bw, bh = box.tolist()\n",
    "\n",
    "        x1 = int((cx - bw / 2) * W)\n",
    "        y1 = int((cy - bh / 2) * H)\n",
    "        x2 = int((cx + bw / 2) * W)\n",
    "        y2 = int((cy + bh / 2) * H)\n",
    "\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(W, x2), min(H, y2)\n",
    "\n",
    "        cropped = original_image[y1:y2, x1:x2]\n",
    "        text = pytesseract.image_to_string(cropped, config='--psm 7')  \n",
    "        \n",
    "        return cropped, text.strip()\n",
    "    \n",
    "# Language selection\n",
    "st.set_page_config(page_title=\"Grounding DINO Streamlit Demo\", layout=\"centered\")\n",
    "lang = st.sidebar.selectbox(\"🌐 Select Language / 언어 선택\", [\"Korean\", \"English\"])\n",
    "\n",
    "# Language-specific text\n",
    "if lang == \"Korean\":\n",
    "    st.title(\"🔍 Grounding DINO 데모\")\n",
    "    st.write(\"이미지를 업로드하고 텍스트 프롬프트에 따라 객체를 탐지해보세요.\")\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.header(\"설정\")\n",
    "        config_path = st.text_input(\"설정 파일 경로\", \"groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "        checkpoint_path = st.text_input(\"체크포인트 파일 경로\", \"/home/bekhzod/Desktop/localization_models_performance/weights/groundingdino_swint_ogc.pth\")\n",
    "        cpu_only = st.checkbox(\"CPU만 사용\", value=False)\n",
    "        box_thresh = st.slider(\"박스 임계값\", 0.0, 1.0, 0.3, 0.05)\n",
    "        text_thresh = st.slider(\"텍스트 임계값\", 0.0, 1.0, 0.3, 0.05)\n",
    "\n",
    "    st.title(\"🧠 차량 번호판 인식 앱\")\n",
    "    text_prompt = st.text_input(\"텍스트 프롬프트\", \"번호판\")\n",
    "    if text_prompt == \"번호판\": text_prompt = \"license plate\"\n",
    "\n",
    "    uploaded_image = st.file_uploader(\"또는 이미지 업로드\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
    "    image_dir = st.text_input(\"이미지 폴더 경로 (선택 사항)\", \"/home/bekhzod/Desktop/localization_models_performance/lp_images/\")\n",
    "\n",
    "else:  # English interface\n",
    "    st.title(\"🔍 Grounding DINO Demo\")\n",
    "    st.write(\"Upload an image and detect objects based on your text prompt.\")\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.header(\"Settings\")\n",
    "        config_path = st.text_input(\"Configuration File Path\", \"groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "        checkpoint_path = st.text_input(\"Checkpoint File Path\", \"weights/groundingdino_swint_ogc.pth\")\n",
    "        cpu_only = st.checkbox(\"Use CPU only\", value=False)\n",
    "        box_thresh = st.slider(\"Box Threshold\", 0.0, 1.0, 0.3, 0.05)\n",
    "        text_thresh = st.slider(\"Text Threshold\", 0.0, 1.0, 0.3, 0.05)\n",
    "\n",
    "    st.title(\"🧠 Vehicle License Plate Recognition App\")\n",
    "    text_prompt = st.text_input(\"Text Prompt\", \"license plate\")\n",
    "\n",
    "    uploaded_image = st.file_uploader(\"Or upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
    "    image_dir = st.text_input(\"Image Folder Path (Optional)\", \"/home/bekhzod/Desktop/localization_models_performance/lp_images/\")\n",
    "\n",
    "# Initialize model\n",
    "device = \"cpu\" if cpu_only else \"cuda\"\n",
    "\n",
    "if os.path.exists(config_path) and os.path.exists(checkpoint_path):\n",
    "    g_dino = GroundingDINOApp(config_path = config_path, checkpoint_path = checkpoint_path, cpu_only = cpu_only, device = device)\n",
    "else:\n",
    "    st.error(\"Please provide valid configuration and checkpoint file paths.\" if lang == \"English\" else \"유효한 설정 파일 및 체크포인트 경로를 입력해주세요.\")\n",
    "    st.stop()\n",
    "\n",
    "# Image preview and selection\n",
    "detection_triggered = False\n",
    "detection_image = None\n",
    "original_cv2 = None\n",
    "result_image = None\n",
    "cropped_img = None\n",
    "ocr_text = \"\"\n",
    "\n",
    "if os.path.isdir(image_dir):\n",
    "    image_paths = glob(os.path.join(image_dir, \"*.[jp][pn]g\"))\n",
    "    random.shuffle(image_paths)\n",
    "    selected_images = image_paths[:10]\n",
    "\n",
    "    st.markdown(\"### 🖼️ Random Image Preview\" if lang == \"English\" else \"### 🖼️ 랜덤 이미지 미리보기\")\n",
    "    rows = [selected_images[i:i+5] for i in range(0, len(selected_images), 5)]\n",
    "    for row in rows:\n",
    "        cols = st.columns(5)\n",
    "        for col, img_path in zip(cols, row):\n",
    "            with col:\n",
    "                pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "                pil_img = ImageOps.fit(pil_img, (200, 200))\n",
    "                st.image(pil_img, caption=os.path.basename(img_path), use_container_width=False)\n",
    "                if st.button(\"Detect from Image\" if lang == \"English\" else \"위 이미지 탐지하기\", key=img_path):\n",
    "                    detection_triggered = True\n",
    "                    detection_image = Image.open(img_path).convert(\"RGB\")\n",
    "                    original_cv2 = np.array(detection_image)\n",
    "elif image_dir.strip():\n",
    "    st.warning(\"Invalid image folder path or folder is empty.\" if lang == \"English\" else \"입력한 이미지 폴더 경로가 잘못되었거나 폴더가 비어있습니다.\")\n",
    "\n",
    "if uploaded_image and not detection_triggered:\n",
    "    detection_image = Image.open(uploaded_image).convert(\"RGB\")\n",
    "    original_cv2 = np.array(detection_image)\n",
    "    detection_triggered = True\n",
    "\n",
    "if detection_triggered and detection_image is not None:\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### 🔍 Detection Results\" if lang == \"English\" else \"### 🔍 탐지 결과\")\n",
    "\n",
    "    st.image(detection_image, caption=\"Original Image\" if lang == \"English\" else \"원본 이미지\", use_container_width=True)\n",
    "\n",
    "    with st.spinner(\"Detecting...\" if lang == \"English\" else \"탐지 중...\"):\n",
    "        _, image_tensor = g_dino.preprocess_image(detection_image)\n",
    "        boxes, phrases = g_dino.get_grounding_output(image_tensor, text_prompt, box_thresh, text_thresh)\n",
    "        boxes = boxes.to(\"cpu\")\n",
    "\n",
    "        result_image = g_dino.plot_boxes(detection_image.copy(), boxes, phrases)\n",
    "        st.image(result_image, caption=\"Detected Results\" if lang == \"English\" else \"탐지된 결과\", use_container_width=True)\n",
    "\n",
    "        if len(boxes) > 0:\n",
    "            cropped_img, ocr_text = g_dino.crop_and_ocr(original_cv2, boxes[0])\n",
    "            st.subheader(\"📝 OCR Result\" if lang == \"English\" else \"📝 OCR 결과\")\n",
    "            st.image(cropped_img, caption=\"Cropped Region\" if lang == \"English\" else \"잘라낸 영역\", use_container_width=True)\n",
    "\n",
    "            cleaned_text = re.sub(r'[^A-Za-z0-9\\- ]', '', ocr_text)\n",
    "            st.success(f\"OCR Result: {cleaned_text}\" if lang == \"English\" else f\"OCR 인식 결과: {cleaned_text}\")\n",
    "        else:\n",
    "            st.warning(\"No object detected.\" if lang == \"English\" else \"탐지된 객체가 없습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localization",
   "language": "python",
   "name": "localization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
