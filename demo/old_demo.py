import os
import cv2
import torch
torch.classes.__path__ = [os.path.join(torch.__path__[0], torch.classes.__file__)] 
# or simply:
# torch.classes.__path__ = []
import sys
sys.path.append("./")
import gc
import numpy as np
import urllib.request
import streamlit as st
from PIL import Image, ImageDraw, ImageFont
from glob import glob
from time import time
import pytesseract
import random  
import re
from PIL import ImageOps

import groundingdino.datasets.transforms as T
from groundingdino.models import build_model
from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap
from groundingdino.util.vl_utils import create_positive_map_from_span
from groundingdino.util.slconfig import SLConfig

class GroundingDINOApp:
    def __init__(self, config_path, checkpoint_path, device, cpu_only=True):
        self.cpu_only = cpu_only        
        self.device   = device
        self.model = self.load_model(config_path, checkpoint_path)
    
    def load_model(self, config_path, checkpoint_path):        
        args = SLConfig.fromfile(config_path)
        args.device = self.device
        model = build_model(args)
        checkpoint = torch.load(checkpoint_path, map_location="cpu")
        model.load_state_dict(clean_state_dict(checkpoint["model"]), strict=False)
        model.eval()
        return model.to(self.device)

    def preprocess_image(self, image_pil):
        transform = T.Compose([
            T.RandomResize([800], max_size=1333),
            T.ToTensor(),
            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])
        image, _ = transform(image_pil, None)
        return image_pil, image.to(self.device)

    def plot_boxes(self, image_pil, boxes, labels):
        W, H = image_pil.size
        draw = ImageDraw.Draw(image_pil)
        font = ImageFont.load_default()

        for box, label in zip(boxes, labels):            
            if torch.get_device(box) != -1: print("Moving boxes to cpu..."); box = box.to("cpu")           
            
            box = box * torch.tensor([W, H, W, H])
            box[:2] -= box[2:] / 2
            box[2:] += box[:2]
            x0, y0, x1, y1 = box.int().tolist()
            color = tuple(np.random.randint(0, 255, size=3).tolist())
            draw.rectangle([x0, y0, x1, y1], outline=color, width=4)
            draw.text((x0, y0), label, fill="white", font=font)
        return image_pil

    def get_grounding_output(self, image_tensor, caption, box_thresh, text_thresh):
        image_tensor = image_tensor.to(self.device)
        caption = caption.strip().lower()
        if not caption.endswith("."):
            caption += "."

        with torch.no_grad():
            outputs = self.model(image_tensor[None], captions=[caption])

        logits = outputs["pred_logits"].sigmoid()[0]
        boxes = outputs["pred_boxes"][0]

        filt_mask = logits.max(dim=1)[0] > box_thresh
        logits_filt = logits[filt_mask]
        boxes_filt = boxes[filt_mask]

        tokenized = self.model.tokenizer(caption)
        pred_phrases = [
            get_phrases_from_posmap(logit > text_thresh, tokenized, self.model.tokenizer) +
            f" ({logit.max().item():.2f})"
            for logit in logits_filt
        ]
        return boxes_filt, pred_phrases

    def crop_and_ocr(self, original_image, box):
        H, W, _ = original_image.shape
        cx, cy, bw, bh = box.tolist()

        x1 = int((cx - bw / 2) * W)
        y1 = int((cy - bh / 2) * H)
        x2 = int((cx + bw / 2) * W)
        y2 = int((cy + bh / 2) * H)

        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(W, x2), min(H, y2)

        cropped = original_image[y1:y2, x1:x2]
        text = pytesseract.image_to_string(cropped, config='--psm 7')  
        
        return cropped, text.strip()
    
# Language selection
st.set_page_config(page_title="Streamlit Demo", layout="centered")
lang = st.sidebar.selectbox("ğŸŒ Select Language / ì–¸ì–´ ì„ íƒ", ["English", "Korean"])

# Language-specific text
if lang == "Korean":
    st.title("ğŸ” ì°¨ëŸ‰ ë²ˆí˜¸íŒ ì¸ì‹ ë°ëª¨")
    st.write("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ê°ì²´ë¥¼ íƒì§€í•´ë³´ì„¸ìš”.")
    mode = st.radio("ëª¨ë“œ ì„ íƒ", ["ì´ë¯¸ì§€", "ë¹„ë””ì˜¤"])

    with st.sidebar:
        st.header("ì„¤ì •")
        config_path = st.text_input("ì„¤ì • íŒŒì¼ ê²½ë¡œ", "groundingdino/config/GroundingDINO_SwinT_OGC.py")
        checkpoint_path = st.text_input("ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ", "groundingdino_swint_ogc.pth")
        cpu_only = st.checkbox("CPUë§Œ ì‚¬ìš©", value=True)
        box_thresh = st.slider("ë°•ìŠ¤ ì„ê³„ê°’", 0.0, 1.0, 0.3, 0.05)
        text_thresh = st.slider("í…ìŠ¤íŠ¸ ì„ê³„ê°’", 0.0, 1.0, 0.3, 0.05)

    
    st.title("ğŸ§  ì°¨ëŸ‰ ë²ˆí˜¸íŒ ì¸ì‹ ì•±")
    text_prompt = st.text_input("í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸", "ë²ˆí˜¸íŒ")
    if text_prompt == "ë²ˆí˜¸íŒ": text_prompt = "license plate"

    uploaded_image = st.file_uploader("ë˜ëŠ” ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["png", "jpg", "jpeg"])
    image_dir = st.text_input("ì´ë¯¸ì§€ í´ë” ê²½ë¡œ (ì„ íƒ ì‚¬í•­)", "lp_images/")

else:  # English interface
    st.title("ğŸ” Licence Plate Detector Demo")
    st.write("Upload an image and detect objects based on your text prompt.")
    mode = st.radio("Select Mode", ["Image", "Video"])

    with st.sidebar:
        st.header("Settings")
        config_path = st.text_input("Configuration File Path", "groundingdino/config/GroundingDINO_SwinT_OGC.py")
        checkpoint_path = st.text_input("Checkpoint File Path", "groundingdino_swint_ogc.pth")
        cpu_only = st.checkbox("Use CPU only", value=True)
        box_thresh = st.slider("Box Threshold", 0.0, 1.0, 0.3, 0.05)
        text_thresh = st.slider("Text Threshold", 0.0, 1.0, 0.3, 0.05)

    st.title("ğŸ§  Vehicle License Plate Recognition App")
    text_prompt = st.text_input("Text Prompt", "license plate")

    uploaded_image = st.file_uploader("Or upload an image", type=["png", "jpg", "jpeg"])
    image_dir = st.text_input("Image Folder Path (Optional)", "lp_images/")

# Initialize model
# device = "cpu" if cpu_only else "cuda"
device = "cpu"
# print(f"device -> {device}")

if not os.path.isfile(checkpoint_path):    
    with st.spinner("Please wait we are downloading the pretrained weights..."):
        urllib.request.urlretrieve(
            "https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth", f"{checkpoint_path}"
        )
    st.success("Pretrained weights have been downloaded!")    



@st.cache_resource
def load_model(): return GroundingDINOApp(config_path=config_path, checkpoint_path=checkpoint_path, cpu_only=cpu_only, device=device)

g_dino = load_model()

@st.cache_data
def get_random_images(image_dir, sample_size=10):
    image_paths = glob(os.path.join(image_dir, "*.[jp][pn]g"))
    random.shuffle(image_paths)
    return image_paths[:sample_size]

detection_triggered = False
detection_image = None
original_cv2 = None
ocr_text = ""
preview_cache = {}

if mode == ("ì´ë¯¸ì§€" if lang == "Korean" else "Image"):
    if os.path.isdir(image_dir):
        selected_images = get_random_images(image_dir)

        st.markdown("### ğŸ–¼ï¸ ëœë¤ ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸°" if lang == "Korean" else "### ğŸ–¼ï¸ Random Image Preview")
        rows = [selected_images[i:i+5] for i in range(0, len(selected_images), 5)]
        for row in rows:
            cols = st.columns(5)
            for col, img_path in zip(cols, row):
                with col:
                    if img_path not in preview_cache:
                        with Image.open(img_path) as pil_img:
                            pil_img = pil_img.convert("RGB")
                            preview_img = ImageOps.fit(pil_img, (200, 200))
                            preview_cache[img_path] = (preview_img, pil_img.copy())
                            pil_img.close()
                    preview_img, full_img = preview_cache[img_path]
                    st.image(preview_img, caption=os.path.basename(img_path), use_container_width=False)

                    if st.button("ìœ„ ì´ë¯¸ì§€ íƒì§€í•˜ê¸°" if lang == "Korean" else "Detect from Image", key=img_path):
                        detection_triggered = True
                        detection_image = full_img
                        original_cv2 = np.array(detection_image)

    elif image_dir.strip():
        st.warning("ì…ë ¥í•œ ì´ë¯¸ì§€ í´ë” ê²½ë¡œê°€ ì˜ëª»ë˜ì—ˆê±°ë‚˜ í´ë”ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤." if lang == "Korean" else "Invalid image folder path or folder is empty.")

    if uploaded_image and not detection_triggered:
        detection_image = Image.open(uploaded_image).convert("RGB")
        original_cv2 = np.array(detection_image)
        detection_triggered = True

    if detection_triggered and detection_image is not None:
        st.markdown("---")
        st.markdown("### ğŸ” íƒì§€ ê²°ê³¼" if lang == "Korean" else "### ğŸ” Detection Results")
        st.image(detection_image, caption="ì›ë³¸ ì´ë¯¸ì§€" if lang == "Korean" else "Original Image", use_container_width=True)

        with st.spinner("íƒì§€ ì¤‘..." if lang == "Korean" else "Detecting..."):
            _, image_tensor = g_dino.preprocess_image(detection_image)
            boxes, phrases = g_dino.get_grounding_output(image_tensor, text_prompt, box_thresh, text_thresh)
            boxes = boxes.to(device)

            result_image = g_dino.plot_boxes(detection_image, boxes, phrases)
            st.image(result_image, caption="íƒì§€ëœ ê²°ê³¼" if lang == "Korean" else "Detected Results", use_container_width=True)

            if len(boxes) > 0:
                cropped_img, ocr_text = g_dino.crop_and_ocr(original_cv2, boxes[0])
                st.subheader("ğŸ“ OCR ê²°ê³¼" if lang == "Korean" else "ğŸ“ OCR Result")
                st.image(cropped_img, caption="ì˜ë¼ë‚¸ ì˜ì—­" if lang == "Korean" else "Cropped Region", use_container_width=True)

                cleaned_text = re.sub(r'[^A-Za-z0-9\- ]', '', ocr_text[:300])
                st.success(f"OCR ì¸ì‹ ê²°ê³¼: {cleaned_text}" if lang == "Korean" else f"OCR Result: {cleaned_text}")
                del cropped_img
            else:
                st.warning("íƒì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤." if lang == "Korean" else "No object detected.")
            del result_image, detection_image, original_cv2, image_tensor
            gc.collect()

else:
    st.markdown("### ğŸï¸ ë¹„ë””ì˜¤ íƒì§€ ëª¨ë“œ" if lang == "Korean" else "### ğŸï¸ Video Detection Mode")

    video_paths = glob(os.path.join(image_dir, "*.mp4"))
    if not video_paths:
        st.warning("í•´ë‹¹ í´ë”ì— mp4 ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤." if lang == "Korean" else "No MP4 videos found in the selected folder.")
    else:
        selected_video = st.selectbox("ë¹„ë””ì˜¤ ì„ íƒ" if lang == "Korean" else "Select a video", video_paths)
        if selected_video and st.button("íƒì§€ ì‹œì‘" if lang == "Korean" else "Start Detection"):
            cap = cv2.VideoCapture(selected_video)
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            st.info(f"FPS: {fps}")
            frame_num = 0
            max_frames = 30  # Limit for low memory

            while cap.isOpened() and frame_num < max_frames:
                ret, frame = cap.read()
                if not ret:
                    break

                if frame_num % fps == 0:  # Process every second
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                    # Resize for memory efficiency
                    max_width = 640
                    if rgb_frame.shape[1] > max_width:
                        scale = max_width / rgb_frame.shape[1]
                        rgb_frame = cv2.resize(rgb_frame, (0, 0), fx=scale, fy=scale)

                    pil_image = Image.fromarray(rgb_frame)

                    with st.spinner(f"{frame_num // fps + 1}ì´ˆ í”„ë ˆì„ íƒì§€ ì¤‘..." if lang == "Korean" else f"Detecting frame at {frame_num // fps + 1} sec..."):
                        _, image_tensor = g_dino.preprocess_image(pil_image)
                        boxes, phrases = g_dino.get_grounding_output(image_tensor, text_prompt, box_thresh, text_thresh)
                        boxes = boxes.to(device)

                        result_image = g_dino.plot_boxes(pil_image, boxes, phrases)
                        st.image(result_image, caption=f"{frame_num // fps + 1}ì´ˆ ê²°ê³¼" if lang == "Korean" else f"Results at {frame_num // fps + 1} sec", use_container_width=True)

                        if len(boxes) > 0:
                            cropped_img, ocr_text = g_dino.crop_and_ocr(frame, boxes[0])
                            cleaned_text = re.sub(r'[^A-Za-z0-9\- ]', '', ocr_text[:300])
                            st.success(f"OCR ì¸ì‹ ê²°ê³¼: {cleaned_text}" if lang == "Korean" else f"OCR Result: {cleaned_text}")
                            del cropped_img
                        else:
                            st.info("íƒì§€ëœ ê°ì²´ ì—†ìŒ" if lang == "Korean" else "No object detected")
                        del result_image, pil_image, image_tensor
                        gc.collect()

                frame_num += 1
            cap.release()